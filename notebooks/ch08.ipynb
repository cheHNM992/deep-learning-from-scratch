{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ch08/deep_convnet.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "sys.path.append(os.pardir)  # 親ディレクトリのファイルをインポートするための設定\n",
    "import pickle\n",
    "import numpy as np\n",
    "from collections import OrderedDict\n",
    "from common.layers import *\n",
    "\n",
    "\n",
    "class DeepConvNet:\n",
    "    \"\"\"認識率99%以上の高精度なConvNet\n",
    "\n",
    "    ネットワーク構成は下記の通り\n",
    "        conv - relu - conv- relu - pool -\n",
    "        conv - relu - conv- relu - pool -\n",
    "        conv - relu - conv- relu - pool -\n",
    "        affine - relu - dropout - affine - dropout - softmax\n",
    "    \"\"\"\n",
    "    def __init__(self, input_dim=(1, 28, 28),\n",
    "                 conv_param_1 = {'filter_num':16, 'filter_size':3, 'pad':1, 'stride':1},\n",
    "                 conv_param_2 = {'filter_num':16, 'filter_size':3, 'pad':1, 'stride':1},\n",
    "                 conv_param_3 = {'filter_num':32, 'filter_size':3, 'pad':1, 'stride':1},\n",
    "                 conv_param_4 = {'filter_num':32, 'filter_size':3, 'pad':2, 'stride':1},\n",
    "                 conv_param_5 = {'filter_num':64, 'filter_size':3, 'pad':1, 'stride':1},\n",
    "                 conv_param_6 = {'filter_num':64, 'filter_size':3, 'pad':1, 'stride':1},\n",
    "                 hidden_size=50, output_size=10):\n",
    "        # 重みの初期化===========\n",
    "        # 各層のニューロンひとつあたりが、前層のニューロンといくつのつながりがあるか（TODO:自動で計算する）\n",
    "        pre_node_nums = np.array([1*3*3, 16*3*3, 16*3*3, 32*3*3, 32*3*3, 64*3*3, 64*4*4, hidden_size])\n",
    "        weight_init_scales = np.sqrt(2.0 / pre_node_nums)  # ReLUを使う場合に推奨される初期値\n",
    "        \n",
    "        self.params = {}\n",
    "        pre_channel_num = input_dim[0]\n",
    "        for idx, conv_param in enumerate([conv_param_1, conv_param_2, conv_param_3, conv_param_4, conv_param_5, conv_param_6]):\n",
    "            self.params['W' + str(idx+1)] = weight_init_scales[idx] * np.random.randn(conv_param['filter_num'], pre_channel_num, conv_param['filter_size'], conv_param['filter_size'])\n",
    "            self.params['b' + str(idx+1)] = np.zeros(conv_param['filter_num'])\n",
    "            pre_channel_num = conv_param['filter_num']\n",
    "        self.params['W7'] = weight_init_scales[6] * np.random.randn(64*4*4, hidden_size)\n",
    "        self.params['b7'] = np.zeros(hidden_size)\n",
    "        self.params['W8'] = weight_init_scales[7] * np.random.randn(hidden_size, output_size)\n",
    "        self.params['b8'] = np.zeros(output_size)\n",
    "\n",
    "        # レイヤの生成===========\n",
    "        self.layers = []\n",
    "        self.layers.append(Convolution(self.params['W1'], self.params['b1'], \n",
    "                           conv_param_1['stride'], conv_param_1['pad']))\n",
    "        self.layers.append(Relu())\n",
    "        self.layers.append(Convolution(self.params['W2'], self.params['b2'], \n",
    "                           conv_param_2['stride'], conv_param_2['pad']))\n",
    "        self.layers.append(Relu())\n",
    "        self.layers.append(Pooling(pool_h=2, pool_w=2, stride=2))\n",
    "        self.layers.append(Convolution(self.params['W3'], self.params['b3'], \n",
    "                           conv_param_3['stride'], conv_param_3['pad']))\n",
    "        self.layers.append(Relu())\n",
    "        self.layers.append(Convolution(self.params['W4'], self.params['b4'],\n",
    "                           conv_param_4['stride'], conv_param_4['pad']))\n",
    "        self.layers.append(Relu())\n",
    "        self.layers.append(Pooling(pool_h=2, pool_w=2, stride=2))\n",
    "        self.layers.append(Convolution(self.params['W5'], self.params['b5'],\n",
    "                           conv_param_5['stride'], conv_param_5['pad']))\n",
    "        self.layers.append(Relu())\n",
    "        self.layers.append(Convolution(self.params['W6'], self.params['b6'],\n",
    "                           conv_param_6['stride'], conv_param_6['pad']))\n",
    "        self.layers.append(Relu())\n",
    "        self.layers.append(Pooling(pool_h=2, pool_w=2, stride=2))\n",
    "        self.layers.append(Affine(self.params['W7'], self.params['b7']))\n",
    "        self.layers.append(Relu())\n",
    "        self.layers.append(Dropout(0.5))\n",
    "        self.layers.append(Affine(self.params['W8'], self.params['b8']))\n",
    "        self.layers.append(Dropout(0.5))\n",
    "        \n",
    "        self.last_layer = SoftmaxWithLoss()\n",
    "\n",
    "    def predict(self, x, train_flg=False):\n",
    "        for layer in self.layers:\n",
    "            if isinstance(layer, Dropout):\n",
    "                x = layer.forward(x, train_flg)\n",
    "            else:\n",
    "                x = layer.forward(x)\n",
    "        return x\n",
    "\n",
    "    def loss(self, x, t):\n",
    "        y = self.predict(x, train_flg=True)\n",
    "        return self.last_layer.forward(y, t)\n",
    "\n",
    "    def accuracy(self, x, t, batch_size=100):\n",
    "        if t.ndim != 1 : t = np.argmax(t, axis=1)\n",
    "\n",
    "        acc = 0.0\n",
    "\n",
    "        for i in range(int(x.shape[0] / batch_size)):\n",
    "            tx = x[i*batch_size:(i+1)*batch_size]\n",
    "            tt = t[i*batch_size:(i+1)*batch_size]\n",
    "            y = self.predict(tx, train_flg=False)\n",
    "            y = np.argmax(y, axis=1)\n",
    "            acc += np.sum(y == tt)\n",
    "\n",
    "        return acc / x.shape[0]\n",
    "\n",
    "    def gradient(self, x, t):\n",
    "        # forward\n",
    "        self.loss(x, t)\n",
    "\n",
    "        # backward\n",
    "        dout = 1\n",
    "        dout = self.last_layer.backward(dout)\n",
    "\n",
    "        tmp_layers = self.layers.copy()\n",
    "        tmp_layers.reverse()\n",
    "        for layer in tmp_layers:\n",
    "            dout = layer.backward(dout)\n",
    "\n",
    "        # 設定\n",
    "        grads = {}\n",
    "        for i, layer_idx in enumerate((0, 2, 5, 7, 10, 12, 15, 18)):\n",
    "            grads['W' + str(i+1)] = self.layers[layer_idx].dW\n",
    "            grads['b' + str(i+1)] = self.layers[layer_idx].db\n",
    "\n",
    "        return grads\n",
    "\n",
    "    def save_params(self, file_name=\"params.pkl\"):\n",
    "        params = {}\n",
    "        for key, val in self.params.items():\n",
    "            params[key] = val\n",
    "        with open(file_name, 'wb') as f:\n",
    "            pickle.dump(params, f)\n",
    "\n",
    "    def load_params(self, file_name=\"params.pkl\"):\n",
    "        with open(file_name, 'rb') as f:\n",
    "            params = pickle.load(f)\n",
    "        for key, val in params.items():\n",
    "            self.params[key] = val\n",
    "\n",
    "        for i, layer_idx in enumerate((0, 2, 5, 7, 10, 12, 15, 18)):\n",
    "            self.layers[layer_idx].W = self.params['W' + str(i+1)]\n",
    "            self.layers[layer_idx].b = self.params['b' + str(i+1)]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ch08/awesome_net.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create your awesome net!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ch08/train_deepnet.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:2.346613058243366\n",
      "=== epoch:1, train acc:0.161, test acc:0.188 ===\n",
      "train loss:2.332607986954287\n",
      "train loss:2.2257382650371493\n",
      "train loss:2.213873750894404\n",
      "train loss:2.2095462190368536\n",
      "train loss:2.1979786520805407\n",
      "train loss:2.2022142628699504\n",
      "train loss:2.1465607623392167\n",
      "train loss:2.213060625135107\n",
      "train loss:2.1297658376558157\n",
      "train loss:2.1256674954891706\n",
      "train loss:2.1229077998264914\n",
      "train loss:2.1760837134170963\n",
      "train loss:2.149723292375601\n",
      "train loss:2.073073798201691\n",
      "train loss:2.0643751661143845\n",
      "train loss:2.0314702771542037\n",
      "train loss:2.0273242695617633\n",
      "train loss:2.0546198461199783\n",
      "train loss:2.0301118594773944\n",
      "train loss:2.0479813542474874\n",
      "train loss:1.883107583492691\n",
      "train loss:1.986029106405969\n",
      "train loss:1.9115585057371924\n",
      "train loss:2.0850206286728166\n",
      "train loss:2.2051799449125387\n",
      "train loss:1.9534268597538251\n",
      "train loss:1.9871099807854633\n",
      "train loss:1.912412306420268\n",
      "train loss:1.96673440526817\n",
      "train loss:1.8373137448388224\n",
      "train loss:2.0949421831755957\n",
      "train loss:2.040867057277771\n",
      "train loss:1.9508507601824645\n",
      "train loss:1.9738688056882596\n",
      "train loss:1.899733788161675\n",
      "train loss:1.9860211576590043\n",
      "train loss:1.8805961786195633\n",
      "train loss:1.9107937651529152\n",
      "train loss:2.068214928498337\n",
      "train loss:1.954740021476056\n",
      "train loss:1.9537134918001562\n",
      "train loss:1.8093214496981218\n",
      "train loss:1.883183614326842\n",
      "train loss:1.8314817248368027\n",
      "train loss:1.788729861602402\n",
      "train loss:1.8051524611078398\n",
      "train loss:1.712809368324131\n",
      "train loss:2.045782307850508\n",
      "train loss:1.8038227768226756\n",
      "train loss:1.7946666866054017\n",
      "=== epoch:2, train acc:0.618, test acc:0.592 ===\n",
      "train loss:1.8573180786684431\n",
      "train loss:1.9114548516109995\n",
      "train loss:1.7768138994666018\n",
      "train loss:1.9299203912489997\n",
      "train loss:1.5901417136230225\n",
      "train loss:1.784920349690689\n",
      "train loss:1.7900636966100794\n",
      "train loss:1.7859022793338357\n",
      "train loss:1.7589687061824026\n",
      "train loss:1.8869111424840648\n",
      "train loss:1.8940645811431205\n",
      "train loss:1.8274525986925394\n",
      "train loss:1.6554144961517434\n",
      "train loss:1.9531608358110881\n",
      "train loss:1.7806691806006152\n",
      "train loss:1.81459775189672\n",
      "train loss:2.003061739876917\n",
      "train loss:1.9268904946905678\n",
      "train loss:1.8218253413929226\n",
      "train loss:1.816629529522437\n",
      "train loss:1.6802838925775136\n",
      "train loss:1.5721749319947405\n",
      "train loss:1.8863557962991118\n",
      "train loss:1.617025066235646\n",
      "train loss:1.9001682650594744\n",
      "train loss:1.553698164084997\n",
      "train loss:1.8029452858885116\n",
      "train loss:2.008794591095752\n",
      "train loss:1.8210988249847082\n",
      "train loss:1.7748523470546926\n",
      "train loss:1.656861187814532\n",
      "train loss:1.6108034813085397\n",
      "train loss:1.6369794479991182\n",
      "train loss:1.8962961605572521\n",
      "train loss:1.6538002833117806\n",
      "train loss:1.9031585456974298\n",
      "train loss:1.5353427726341613\n",
      "train loss:1.8890200249921096\n",
      "train loss:1.720149206350483\n",
      "train loss:1.6675950111485998\n",
      "train loss:1.7844815738327697\n",
      "train loss:1.848500340861969\n",
      "train loss:1.8452502976314493\n",
      "train loss:1.8035710492357218\n",
      "train loss:1.7241009421914415\n",
      "train loss:1.785512185091089\n",
      "train loss:1.7879075465563972\n",
      "train loss:1.643685771134984\n",
      "train loss:1.7518066685048235\n",
      "train loss:1.7982738281836321\n",
      "=== epoch:3, train acc:0.767, test acc:0.73 ===\n",
      "train loss:1.8164337956167076\n",
      "train loss:1.6964354522866847\n",
      "train loss:1.7825337935757284\n",
      "train loss:1.6105749317757525\n",
      "train loss:1.6919467218759383\n",
      "train loss:1.6415582112110791\n",
      "train loss:1.6058177918003909\n",
      "train loss:1.6304350529204796\n",
      "train loss:1.6694732922297246\n",
      "train loss:1.5901714507365117\n",
      "train loss:1.5959706314597366\n",
      "train loss:1.7165394036337098\n",
      "train loss:1.7202740860813597\n",
      "train loss:1.6921837914068245\n",
      "train loss:1.7281827938133936\n",
      "train loss:1.6068665239950344\n",
      "train loss:1.669045563756275\n",
      "train loss:1.7774404762651101\n",
      "train loss:1.8489746813459218\n",
      "train loss:1.680235000254691\n",
      "train loss:1.5553822457393967\n",
      "train loss:1.6459015190661068\n",
      "train loss:1.74370456221586\n",
      "train loss:1.7433033081563674\n",
      "train loss:1.688276032591708\n",
      "train loss:1.5487783362928227\n",
      "train loss:1.6288411945861814\n",
      "train loss:1.648823338123775\n",
      "train loss:1.6213800862759806\n",
      "train loss:1.4689619293354161\n",
      "train loss:1.7061433056646154\n",
      "train loss:1.7959463520116241\n",
      "train loss:1.6765424252974699\n",
      "train loss:1.6518717420360238\n",
      "train loss:1.4321616682383906\n",
      "train loss:1.630061432351863\n",
      "train loss:1.645260063281036\n",
      "train loss:1.6810344928636054\n",
      "train loss:1.3844679145388323\n",
      "train loss:1.6263354789204223\n",
      "train loss:1.6180462004921767\n",
      "train loss:1.6995481920177369\n",
      "train loss:1.779253170652065\n",
      "train loss:1.7260535808669977\n",
      "train loss:1.732118510330363\n",
      "train loss:1.491957627519971\n",
      "train loss:1.6025885689043076\n",
      "train loss:1.6364360243175506\n",
      "train loss:1.5964455705277008\n",
      "train loss:1.5836538179503135\n",
      "=== epoch:4, train acc:0.805, test acc:0.762 ===\n",
      "train loss:1.7632760047545653\n",
      "train loss:1.6658550358245987\n",
      "train loss:1.5712154993126897\n",
      "train loss:1.5213247592092956\n",
      "train loss:1.456239220551078\n",
      "train loss:1.456639277703372\n",
      "train loss:1.675829097042302\n",
      "train loss:1.5463619882162545\n",
      "train loss:1.547024668105532\n",
      "train loss:1.5701253363883851\n",
      "train loss:1.5019104812471107\n",
      "train loss:1.4962286015770718\n",
      "train loss:1.6815502970102718\n",
      "train loss:1.6639124765763247\n",
      "train loss:1.6197708632938554\n",
      "train loss:1.6390470110243414\n",
      "train loss:1.5902452949422394\n",
      "train loss:1.5492132416058901\n",
      "train loss:1.6493045157774326\n",
      "train loss:1.7509743966157112\n",
      "train loss:1.6700468173960799\n",
      "train loss:1.5608393021223512\n",
      "train loss:1.7199234647306276\n",
      "train loss:1.5062293337206833\n",
      "train loss:1.5731690653310038\n",
      "train loss:1.5466743974818138\n",
      "train loss:1.464092564534977\n",
      "train loss:1.5819001092218317\n",
      "train loss:1.714172733518481\n",
      "train loss:1.2930882636703063\n",
      "train loss:1.465585128340776\n",
      "train loss:1.588117417074711\n",
      "train loss:1.515405505697589\n",
      "train loss:1.4382615498327738\n",
      "train loss:1.5480691565683895\n",
      "train loss:1.6704487383365216\n",
      "train loss:1.4598717163874328\n",
      "train loss:1.6987229772402117\n",
      "train loss:1.5050562973018393\n",
      "train loss:1.3115411431042412\n",
      "train loss:1.2694947169452357\n",
      "train loss:1.4063804564205875\n",
      "train loss:1.6834809806940618\n",
      "train loss:1.551079512730815\n",
      "train loss:1.6089451365047502\n",
      "train loss:1.571134196453388\n",
      "train loss:1.5180029699079192\n",
      "train loss:1.4701344562424057\n",
      "train loss:1.4630812605650183\n",
      "train loss:1.5688096654183163\n",
      "=== epoch:5, train acc:0.788, test acc:0.764 ===\n",
      "train loss:1.6428113362059173\n",
      "train loss:1.544564527420833\n",
      "train loss:1.4573466421635854\n",
      "train loss:1.550490699668289\n",
      "train loss:1.4206479272795942\n",
      "train loss:1.45037496707019\n",
      "train loss:1.5806715651946857\n",
      "train loss:1.522276797844421\n",
      "train loss:1.5173808150668007\n",
      "train loss:1.4250929402718011\n",
      "train loss:1.733496588033634\n",
      "train loss:1.4818511505116558\n",
      "train loss:1.6299032148502248\n",
      "train loss:1.601126254371248\n",
      "train loss:1.5030425900303184\n",
      "train loss:1.6605688505625842\n",
      "train loss:1.6027893062600542\n",
      "train loss:1.7818254509876608\n",
      "train loss:1.5481579152727414\n",
      "train loss:1.4867122188175381\n",
      "train loss:1.5383371788891482\n",
      "train loss:1.6117356535350313\n",
      "train loss:1.6146155944963463\n",
      "train loss:1.5063122649519851\n",
      "train loss:1.4961188303254795\n",
      "train loss:1.4869908791940492\n",
      "train loss:1.300194934515834\n",
      "train loss:1.680659812746491\n",
      "train loss:1.5104493034028883\n",
      "train loss:1.5468475441686806\n",
      "train loss:1.4838236225001307\n",
      "train loss:1.4513338224008272\n",
      "train loss:1.5040083138060945\n",
      "train loss:1.469513227301268\n",
      "train loss:1.317130498388999\n",
      "train loss:1.5658383703957173\n",
      "train loss:1.4297960938290126\n",
      "train loss:1.3937191215617861\n",
      "train loss:1.5132194648627613\n",
      "train loss:1.5287606873853081\n",
      "train loss:1.4370606386206393\n",
      "train loss:1.5841341547581007\n",
      "train loss:1.457339813254579\n",
      "train loss:1.5030295533461446\n",
      "train loss:1.5739880046318484\n",
      "train loss:1.4431311242050184\n",
      "train loss:1.4029013156952501\n",
      "train loss:1.6489924446863438\n",
      "train loss:1.4397258208719887\n",
      "train loss:1.5285002564286916\n",
      "=== epoch:6, train acc:0.829, test acc:0.797 ===\n",
      "train loss:1.6265474589410196\n",
      "train loss:1.4725446607748662\n",
      "train loss:1.3993232258112502\n",
      "train loss:1.5692630070109512\n",
      "train loss:1.3614567000769642\n",
      "train loss:1.5031455359399386\n",
      "train loss:1.4923005941845904\n",
      "train loss:1.4658102670304367\n",
      "train loss:1.5546540165459364\n",
      "train loss:1.4178746536015143\n",
      "train loss:1.504629641030064\n",
      "train loss:1.389275350193329\n",
      "train loss:1.3343489129692483\n",
      "train loss:1.5422790143900773\n",
      "train loss:1.56444933606493\n",
      "train loss:1.5137022178930253\n",
      "train loss:1.3268274337676909\n",
      "train loss:1.603465810201354\n",
      "train loss:1.2736770291358477\n",
      "train loss:1.526154599608389\n",
      "train loss:1.4815096938188348\n",
      "train loss:1.4557114702927572\n",
      "train loss:1.5566308425604671\n",
      "train loss:1.5263494200757528\n",
      "train loss:1.3696779206246434\n",
      "train loss:1.4354029737490066\n",
      "train loss:1.4972534558122512\n",
      "train loss:1.4475275496214817\n",
      "train loss:1.513584227646132\n",
      "train loss:1.5374675443851749\n",
      "train loss:1.5493086415635486\n",
      "train loss:1.3894709671033736\n",
      "train loss:1.3035721672824485\n",
      "train loss:1.3587141759863552\n",
      "train loss:1.3836077308553356\n",
      "train loss:1.4329444267204443\n",
      "train loss:1.5498793765877317\n",
      "train loss:1.6667464411695587\n",
      "train loss:1.5290507359624963\n",
      "train loss:1.4141000062652105\n",
      "train loss:1.4093232855929\n",
      "train loss:1.349621308673345\n",
      "train loss:1.4874610398826096\n",
      "train loss:1.472633709238488\n",
      "train loss:1.6598640531493092\n",
      "train loss:1.4533570688723898\n",
      "train loss:1.523336065236944\n",
      "train loss:1.3992338764013073\n",
      "train loss:1.4050093863887134\n",
      "train loss:1.2709243995256454\n",
      "=== epoch:7, train acc:0.838, test acc:0.813 ===\n",
      "train loss:1.3134862977561825\n",
      "train loss:1.5021637476232599\n",
      "train loss:1.566778842888363\n",
      "train loss:1.308556442769983\n",
      "train loss:1.2840786876544994\n",
      "train loss:1.5147001968877962\n",
      "train loss:1.4280148974403957\n",
      "train loss:1.5165858524663647\n",
      "train loss:1.4686380808833375\n",
      "train loss:1.467845617474043\n",
      "train loss:1.3495497121300222\n",
      "train loss:1.3283776926527915\n",
      "train loss:1.564891738736098\n",
      "train loss:1.2989091972769982\n",
      "train loss:1.427680568290527\n",
      "train loss:1.628986275873321\n",
      "train loss:1.5030418919228286\n",
      "train loss:1.260832798697644\n",
      "train loss:1.5236531183951436\n",
      "train loss:1.3023524116222853\n",
      "train loss:1.3268869296985397\n",
      "train loss:1.4779288157331731\n",
      "train loss:1.5951636698618075\n",
      "train loss:1.4477547086466145\n",
      "train loss:1.5110980116310389\n",
      "train loss:1.5419963035524384\n",
      "train loss:1.4525924629417188\n",
      "train loss:1.3953063787007516\n",
      "train loss:1.4624019098771432\n",
      "train loss:1.3125574556692385\n",
      "train loss:1.1200479310204448\n",
      "train loss:1.4548929134499857\n",
      "train loss:1.386615308915855\n",
      "train loss:1.30985606260122\n",
      "train loss:1.3787422494450143\n",
      "train loss:1.2170616922449495\n",
      "train loss:1.362511642268898\n",
      "train loss:1.414131013069342\n",
      "train loss:1.396561588481587\n",
      "train loss:1.3426382592522161\n",
      "train loss:1.2320920480715771\n",
      "train loss:1.5180971499909672\n",
      "train loss:1.5163553955444125\n",
      "train loss:1.2338527733240803\n",
      "train loss:1.1240533000169577\n",
      "train loss:1.401497446483103\n",
      "train loss:1.354391694527377\n",
      "train loss:1.4046727168026498\n",
      "train loss:1.331896026179164\n",
      "train loss:1.3944977195647332\n",
      "=== epoch:8, train acc:0.848, test acc:0.794 ===\n",
      "train loss:1.1817136617276194\n",
      "train loss:1.4548569104935343\n",
      "train loss:1.2257073765003597\n",
      "train loss:1.4521033359757738\n",
      "train loss:1.4539754511184972\n",
      "train loss:1.2782989512688026\n",
      "train loss:1.3171442644053541\n",
      "train loss:1.2808471346272545\n",
      "train loss:1.2790504673997576\n",
      "train loss:1.667914214415191\n",
      "train loss:1.4040988681988238\n",
      "train loss:1.3249795609428312\n",
      "train loss:1.5863051811673232\n",
      "train loss:1.30955700060499\n",
      "train loss:1.4837050185510392\n",
      "train loss:1.1981418033264202\n",
      "train loss:1.372205571017921\n",
      "train loss:1.6524988516112145\n",
      "train loss:1.2922973801578863\n",
      "train loss:1.4653872663113026\n",
      "train loss:1.3243039789142403\n",
      "train loss:1.5005708604273045\n",
      "train loss:1.343109373197426\n",
      "train loss:1.3426855344234034\n",
      "train loss:1.455266373049519\n",
      "train loss:1.5038824081961466\n",
      "train loss:1.6210442256831623\n",
      "train loss:1.359363814733121\n",
      "train loss:1.427887506629392\n",
      "train loss:1.4296547807651516\n",
      "train loss:1.3295031643890798\n",
      "train loss:1.259560592996493\n",
      "train loss:1.5602724286376042\n",
      "train loss:1.3706098553179678\n",
      "train loss:1.2662686199172013\n",
      "train loss:1.4247033980592605\n",
      "train loss:1.4064904416449653\n",
      "train loss:1.3530545491741928\n",
      "train loss:1.3087042779830163\n",
      "train loss:1.3601762864238238\n",
      "train loss:1.4355397095284224\n",
      "train loss:1.3897347293290871\n",
      "train loss:1.3129006271368935\n",
      "train loss:1.4187149823850305\n",
      "train loss:1.430057003234059\n",
      "train loss:1.4055970464435066\n",
      "train loss:1.2209016576924958\n",
      "train loss:1.4943090843604647\n",
      "train loss:1.2514293120508195\n",
      "train loss:1.4945529369724275\n",
      "=== epoch:9, train acc:0.86, test acc:0.81 ===\n",
      "train loss:1.3210420998675219\n",
      "train loss:1.3138894603876798\n",
      "train loss:1.3703908808981913\n",
      "train loss:1.362774838270118\n",
      "train loss:1.3939940220459244\n",
      "train loss:1.2350727278797538\n",
      "train loss:1.5641603024279127\n",
      "train loss:1.2968701434240402\n",
      "train loss:1.2917028721188302\n",
      "train loss:1.4743718757292126\n",
      "train loss:1.2052223586837256\n",
      "train loss:1.257814222822731\n",
      "train loss:1.3943720019409205\n",
      "train loss:1.2237745560491609\n",
      "train loss:1.346166168487762\n",
      "train loss:1.0813885448427734\n",
      "train loss:1.3162787115882322\n",
      "train loss:1.2601540858522897\n",
      "train loss:1.3899087011477236\n",
      "train loss:1.4663270644047972\n",
      "train loss:1.4073153226641384\n",
      "train loss:1.2156449368318674\n",
      "train loss:1.313331544784941\n",
      "train loss:1.540218974206458\n",
      "train loss:1.2621325528674106\n",
      "train loss:1.2335193195945062\n",
      "train loss:1.3612515159580516\n",
      "train loss:1.243541253923154\n",
      "train loss:1.4338411236489228\n",
      "train loss:1.3091119631414398\n",
      "train loss:1.3528452028537035\n",
      "train loss:1.5071131657258605\n",
      "train loss:1.3052820418994586\n",
      "train loss:1.3184573456541033\n",
      "train loss:1.4038202140358658\n",
      "train loss:1.203599593189332\n",
      "train loss:1.2935048291227842\n",
      "train loss:1.135731445562222\n",
      "train loss:1.3532431255664215\n",
      "train loss:1.2935621333445604\n",
      "train loss:1.1325141780505665\n",
      "train loss:1.2650240328552302\n",
      "train loss:1.4790211612022748\n",
      "train loss:1.424621215193951\n",
      "train loss:1.2371847201906812\n",
      "train loss:1.2360529110537113\n",
      "train loss:1.1606356931214206\n",
      "train loss:1.4361442360390706\n",
      "train loss:1.391300240025969\n",
      "train loss:1.3978437487740962\n",
      "=== epoch:10, train acc:0.868, test acc:0.825 ===\n",
      "train loss:1.3067641405836443\n",
      "train loss:1.3689548290280227\n",
      "train loss:1.3111622189320298\n",
      "train loss:1.3572988769353116\n",
      "train loss:1.1637124427589938\n",
      "train loss:1.4187756236389588\n",
      "train loss:1.2662474789325915\n",
      "train loss:1.3673965498634804\n",
      "train loss:1.2284172345635247\n",
      "train loss:1.3317415202181173\n",
      "train loss:1.3584410072738091\n",
      "train loss:1.2565846864675965\n",
      "train loss:1.2724444758501032\n",
      "train loss:1.3368991188637336\n",
      "train loss:1.4324672498304432\n",
      "train loss:1.4120364545682693\n",
      "train loss:1.18486638137068\n",
      "train loss:1.4373465476941791\n",
      "train loss:1.2319029053746278\n",
      "train loss:1.3389224030338405\n",
      "train loss:1.3642235373815217\n",
      "train loss:1.272097004233982\n",
      "train loss:1.406916076589006\n",
      "train loss:1.2383468914198525\n",
      "train loss:1.3863946823211135\n",
      "train loss:1.1329370222317934\n",
      "train loss:1.3658396997353028\n",
      "train loss:1.3835037824261887\n",
      "train loss:1.4045240402571408\n",
      "train loss:1.2939743453868007\n",
      "train loss:1.4838187026621217\n",
      "train loss:1.504605347773655\n",
      "train loss:1.1605330550597224\n",
      "train loss:1.1112758328263226\n",
      "train loss:1.160370632947662\n",
      "train loss:1.346721557956694\n",
      "train loss:1.3962268752009661\n",
      "train loss:1.231064780459003\n",
      "train loss:1.1267476868484594\n",
      "train loss:1.1434063002291923\n",
      "train loss:1.2767482867928261\n",
      "train loss:1.4049869259792838\n",
      "train loss:1.3650331808423095\n",
      "train loss:1.2178130808972274\n",
      "train loss:1.408110873423018\n",
      "train loss:1.2694956947169513\n",
      "train loss:1.2693893718407987\n",
      "train loss:1.4580845377851464\n",
      "train loss:1.253451387964032\n",
      "train loss:1.3991880643505936\n",
      "=== epoch:11, train acc:0.875, test acc:0.836 ===\n",
      "train loss:1.2181753149618588\n",
      "train loss:1.1880218724604057\n",
      "train loss:1.2119258640860555\n",
      "train loss:1.271719836907284\n",
      "train loss:1.477444522515668\n",
      "train loss:1.1843233610366775\n",
      "train loss:1.3743623989264047\n",
      "train loss:1.464349181594596\n",
      "train loss:1.3760102355638193\n",
      "train loss:1.4508278750382793\n",
      "train loss:1.4956754166268078\n",
      "train loss:1.2719023267399643\n",
      "train loss:1.2007202708733022\n",
      "train loss:1.4414907475048753\n",
      "train loss:1.3042723310975994\n",
      "train loss:1.3480914194736033\n",
      "train loss:1.361287005626172\n",
      "train loss:1.0501055665756869\n",
      "train loss:1.3836604093317195\n",
      "train loss:1.412644918642332\n",
      "train loss:1.240978897584264\n",
      "train loss:1.3395819721414617\n",
      "train loss:1.313260608741754\n",
      "train loss:1.2171263030814035\n",
      "train loss:1.2193365200574455\n",
      "train loss:1.235458895625466\n",
      "train loss:1.161249505491597\n",
      "train loss:1.3881347116232328\n",
      "train loss:1.1963891048885467\n",
      "train loss:1.1907754375553854\n",
      "train loss:1.4220308041019427\n",
      "train loss:1.2507231062208297\n",
      "train loss:1.2931948735136465\n",
      "train loss:1.3354994469844197\n",
      "train loss:1.2280173000211496\n",
      "train loss:1.3213664178922093\n",
      "train loss:1.2954896789970975\n",
      "train loss:1.1128874144578804\n",
      "train loss:1.2078347254236865\n",
      "train loss:1.3289338027962672\n",
      "train loss:1.1216272036787152\n",
      "train loss:1.31491704683348\n",
      "train loss:1.3580333729000806\n",
      "train loss:1.1696654969332954\n",
      "train loss:1.410863124877105\n",
      "train loss:1.0150101504576081\n",
      "train loss:1.3705277000790297\n",
      "train loss:1.183822639438337\n",
      "train loss:1.1815534228251119\n",
      "train loss:1.6666914659692411\n",
      "=== epoch:12, train acc:0.878, test acc:0.842 ===\n",
      "train loss:1.2853915147365438\n",
      "train loss:1.367124323008586\n",
      "train loss:1.2861427765241447\n",
      "train loss:1.2176543033946945\n",
      "train loss:1.23296437424006\n",
      "train loss:1.4167234585156265\n",
      "train loss:1.2395456200068256\n",
      "train loss:1.3849623613622162\n",
      "train loss:1.3448103960570439\n",
      "train loss:1.3379684262730303\n",
      "train loss:1.308439642566301\n",
      "train loss:1.179054054884541\n",
      "train loss:1.2591070826393194\n",
      "train loss:1.2198521229889\n",
      "train loss:1.2642939782794236\n",
      "train loss:1.488254498738663\n",
      "train loss:1.2275706589554118\n",
      "train loss:1.3770696057286835\n",
      "train loss:1.240495346085841\n",
      "train loss:1.1010604707724203\n",
      "train loss:1.26079605577954\n",
      "train loss:1.348125258550771\n",
      "train loss:1.0792430966794668\n",
      "train loss:1.2669641152435045\n",
      "train loss:1.279105272800161\n",
      "train loss:1.4373096038740472\n",
      "train loss:1.2707122436851348\n",
      "train loss:1.1601803727773166\n",
      "train loss:1.1122763983658228\n",
      "train loss:1.1505343750075376\n",
      "train loss:1.3055423270656377\n",
      "train loss:1.3215380310324836\n",
      "train loss:1.3112793719131182\n",
      "train loss:1.538356238913896\n",
      "train loss:1.1581911860278493\n",
      "train loss:1.4059492741536528\n",
      "train loss:1.2025188071047666\n",
      "train loss:1.3412554677822348\n",
      "train loss:1.3641641722150613\n",
      "train loss:1.4806971126156103\n",
      "train loss:1.2212055738917298\n",
      "train loss:1.3043100177770135\n",
      "train loss:1.2489775662572717\n",
      "train loss:1.1253343637011195\n",
      "train loss:1.1845248440594274\n",
      "train loss:1.047781622110805\n",
      "train loss:1.2402553710059658\n",
      "train loss:1.3769656919219906\n",
      "train loss:1.4443232536502788\n",
      "train loss:1.320331484804348\n",
      "=== epoch:13, train acc:0.861, test acc:0.825 ===\n",
      "train loss:1.2645721714233868\n",
      "train loss:1.257550506342735\n",
      "train loss:1.2730092766289256\n",
      "train loss:1.2566720516981271\n",
      "train loss:1.2976489585070052\n",
      "train loss:1.2537120339652401\n",
      "train loss:1.35304891914114\n",
      "train loss:1.1880599701301342\n",
      "train loss:1.1419685757014728\n",
      "train loss:1.233930144233267\n",
      "train loss:1.3270303532877015\n",
      "train loss:1.4142138367321766\n",
      "train loss:1.200902004845544\n",
      "train loss:1.22375477423023\n",
      "train loss:1.2462134142340195\n",
      "train loss:1.249864701142353\n",
      "train loss:1.2935930659272616\n",
      "train loss:1.1604759169258965\n",
      "train loss:1.2096309208740201\n",
      "train loss:1.1365117039435682\n",
      "train loss:1.132047569592058\n",
      "train loss:1.279376274583947\n",
      "train loss:1.2855472579135057\n",
      "train loss:1.1796043168505013\n",
      "train loss:1.3333408340754664\n",
      "train loss:1.4039276340406253\n",
      "train loss:1.2369602634342305\n",
      "train loss:1.218701049765518\n",
      "train loss:1.2274479787101802\n",
      "train loss:1.2684622115154522\n",
      "train loss:1.2814371698940834\n",
      "train loss:1.379419894290913\n",
      "train loss:1.0953806208158057\n",
      "train loss:1.284893346990951\n",
      "train loss:1.1119282723323316\n",
      "train loss:1.326347596427827\n",
      "train loss:1.1737146667055571\n",
      "train loss:1.383583745899206\n",
      "train loss:1.2005290628531295\n",
      "train loss:1.1921923885334313\n",
      "train loss:1.1633711397742166\n",
      "train loss:1.2600613012227253\n",
      "train loss:1.1184213820811515\n",
      "train loss:1.3672508711944908\n",
      "train loss:1.3530339610546982\n",
      "train loss:1.4289317967522417\n",
      "train loss:1.01518077389674\n",
      "train loss:1.177682694832908\n",
      "train loss:1.1307598209137213\n",
      "train loss:1.0895811179433306\n",
      "=== epoch:14, train acc:0.883, test acc:0.841 ===\n",
      "train loss:1.2451646978738218\n",
      "train loss:1.2254582181633054\n",
      "train loss:1.1208880597944397\n",
      "train loss:1.143324012982792\n",
      "train loss:1.186933512356835\n",
      "train loss:1.2323865515548134\n",
      "train loss:1.2294651925242608\n",
      "train loss:1.3235795782906827\n",
      "train loss:1.3216600472768292\n",
      "train loss:1.0968321217567087\n",
      "train loss:1.1424876568266589\n",
      "train loss:1.2198969263939872\n",
      "train loss:1.275079466198557\n",
      "train loss:1.1308664337683294\n",
      "train loss:1.3751610622803634\n",
      "train loss:1.3850015789826398\n",
      "train loss:1.2912610420733786\n",
      "train loss:1.2461781189417915\n",
      "train loss:1.3255531695101146\n",
      "train loss:1.1108002318813066\n",
      "train loss:1.278594902224396\n",
      "train loss:1.2767546671951098\n",
      "train loss:1.2558361034324748\n",
      "train loss:1.302763165246323\n",
      "train loss:1.2391323725827426\n",
      "train loss:1.3561768096489424\n",
      "train loss:1.2804468054528075\n",
      "train loss:1.1718195246526812\n",
      "train loss:1.1710386563994128\n",
      "train loss:1.1371080899080876\n",
      "train loss:1.1828546437825962\n",
      "train loss:1.3283309632864553\n",
      "train loss:1.406514636879894\n",
      "train loss:1.1067290921011483\n",
      "train loss:1.122343411246233\n",
      "train loss:1.1004259620894263\n",
      "train loss:1.1733191013886877\n",
      "train loss:1.2648178548275897\n",
      "train loss:1.4128641773602835\n",
      "train loss:1.3165155462664029\n",
      "train loss:1.3141738839451969\n",
      "train loss:1.1644889793556306\n",
      "train loss:0.9803966410656949\n",
      "train loss:1.1549718078838958\n",
      "train loss:1.2732408775453319\n",
      "train loss:1.2870937581547646\n",
      "train loss:1.1812804992567587\n",
      "train loss:1.1710061500219149\n",
      "train loss:1.1489856762200557\n",
      "train loss:1.2597133438587742\n",
      "=== epoch:15, train acc:0.876, test acc:0.842 ===\n",
      "train loss:1.21286177290471\n",
      "train loss:1.3852000835681695\n",
      "train loss:1.2589314089097234\n",
      "train loss:1.212663812995587\n",
      "train loss:1.1095890142148568\n",
      "train loss:1.175413035581209\n",
      "train loss:1.1000659822143322\n",
      "train loss:1.2021302118699821\n",
      "train loss:1.1180084997336914\n",
      "train loss:1.1737138857702023\n",
      "train loss:1.204130442846179\n",
      "train loss:1.4276944192852055\n",
      "train loss:1.282634330624951\n",
      "train loss:1.207327986887189\n",
      "train loss:1.1849303497179682\n",
      "train loss:1.1863683169296544\n",
      "train loss:1.2251832130843727\n",
      "train loss:1.3351854750160337\n",
      "train loss:1.1914627249266818\n",
      "train loss:1.1724356198675727\n",
      "train loss:1.2857767705206846\n",
      "train loss:1.1466429674521346\n",
      "train loss:1.2555037032296132\n",
      "train loss:1.4271084777301217\n",
      "train loss:1.2978919621119231\n",
      "train loss:1.0168078330840524\n",
      "train loss:1.1669014713374886\n",
      "train loss:1.1890295262355302\n",
      "train loss:1.2390025638752176\n",
      "train loss:1.1308038990586844\n",
      "train loss:1.3344011201497994\n",
      "train loss:1.1628870239919422\n",
      "train loss:1.1722222618350622\n",
      "train loss:1.2640195431057812\n",
      "train loss:1.396294972240369\n",
      "train loss:1.1693347306214101\n",
      "train loss:1.1910765324270105\n",
      "train loss:1.1684035700228037\n",
      "train loss:1.2982354780848084\n",
      "train loss:1.1729005152555867\n",
      "train loss:1.3680417450103033\n",
      "train loss:1.1134409091634\n",
      "train loss:1.3369024203596607\n",
      "train loss:1.167316642436424\n",
      "train loss:1.2082969791230804\n",
      "train loss:1.1763928349657633\n",
      "train loss:1.21484850468661\n",
      "train loss:1.2326368236762537\n",
      "train loss:1.1599456554372314\n",
      "train loss:1.2186525883125432\n",
      "=== epoch:16, train acc:0.886, test acc:0.843 ===\n",
      "train loss:1.1727295689939847\n",
      "train loss:1.2656913884028118\n",
      "train loss:1.156144921267301\n",
      "train loss:1.1773120180709062\n",
      "train loss:1.2027617534428723\n",
      "train loss:1.2609728683172046\n",
      "train loss:1.1234017214536158\n",
      "train loss:1.0800315697106706\n",
      "train loss:1.2779514592652488\n",
      "train loss:1.4842046542605882\n",
      "train loss:1.3195053414937445\n",
      "train loss:1.2452587527308214\n",
      "train loss:1.0923696425107223\n",
      "train loss:1.2845625408115109\n",
      "train loss:1.0764734950841244\n",
      "train loss:1.1144320407771213\n",
      "train loss:1.1697756366459606\n",
      "train loss:1.2444823227876978\n",
      "train loss:1.2901264870139508\n",
      "train loss:1.1261425183961105\n",
      "train loss:1.3176739520793075\n",
      "train loss:1.3516877860011578\n",
      "train loss:1.4036553606274762\n",
      "train loss:1.1474855479610377\n",
      "train loss:1.367185670967621\n",
      "train loss:1.1527763902166341\n",
      "train loss:1.1932807690754774\n",
      "train loss:1.1911461736400668\n",
      "train loss:1.2307256811361633\n",
      "train loss:1.2224675876564162\n",
      "train loss:1.3137108596621243\n",
      "train loss:1.1512282348903613\n",
      "train loss:1.2008259294569308\n",
      "train loss:1.1913008103143097\n",
      "train loss:1.3789434709211081\n",
      "train loss:1.1266802202608774\n",
      "train loss:1.1785217592636097\n",
      "train loss:1.0311153422214827\n",
      "train loss:1.128651984103702\n",
      "train loss:1.220953704362797\n",
      "train loss:1.1139749810932842\n",
      "train loss:1.2703536995193259\n",
      "train loss:1.2203469862995973\n",
      "train loss:1.2952085184406041\n",
      "train loss:1.1764433520374182\n",
      "train loss:1.3086927512922375\n",
      "train loss:1.1287603263522565\n",
      "train loss:1.2445538532555132\n",
      "train loss:1.2091332563315442\n",
      "train loss:1.2315969037586765\n",
      "=== epoch:17, train acc:0.896, test acc:0.852 ===\n",
      "train loss:1.4729621214749844\n",
      "train loss:1.278315927507599\n",
      "train loss:0.9950281339748895\n",
      "train loss:1.1750747161524733\n",
      "train loss:1.2009167152217328\n",
      "train loss:1.2014079090605514\n",
      "train loss:1.2928539932271406\n",
      "train loss:1.054309672008384\n",
      "train loss:1.1759164898530168\n",
      "train loss:1.1882858487161525\n",
      "train loss:1.376224088657466\n",
      "train loss:1.1918507607910922\n",
      "train loss:1.136053034200164\n",
      "train loss:1.4783191955219666\n",
      "train loss:1.186506297150463\n",
      "train loss:1.2646439820792061\n",
      "train loss:1.2855540108928067\n",
      "train loss:1.1910365196299744\n",
      "train loss:1.122932974359158\n",
      "train loss:1.2036742472044673\n",
      "train loss:1.214940145226077\n",
      "train loss:1.279408798760766\n",
      "train loss:1.2484942862446777\n",
      "train loss:1.249302499827752\n",
      "train loss:1.1576203828296077\n",
      "train loss:1.175469462275576\n",
      "train loss:1.087379422023123\n",
      "train loss:1.407771142589854\n",
      "train loss:0.9710364368738142\n",
      "train loss:1.2658021594864812\n",
      "train loss:1.2630246309666113\n",
      "train loss:1.3000040727300537\n",
      "train loss:1.0140639151283783\n",
      "train loss:1.0987543241543418\n",
      "train loss:1.1618599045463198\n",
      "train loss:1.3463281802538813\n",
      "train loss:1.2794725297401675\n",
      "train loss:1.1507376172537966\n",
      "train loss:1.1999956059846044\n",
      "train loss:1.0988786726799555\n",
      "train loss:1.0781337516002958\n",
      "train loss:1.1929227063276215\n",
      "train loss:1.0514525937847647\n",
      "train loss:1.1153639929186923\n",
      "train loss:1.178991694193612\n",
      "train loss:1.1977008679545893\n",
      "train loss:1.2252034962198117\n",
      "train loss:1.1501106680099045\n",
      "train loss:1.2490407273454311\n",
      "train loss:1.1525827637819583\n",
      "=== epoch:18, train acc:0.898, test acc:0.84 ===\n",
      "train loss:1.1140126649072493\n",
      "train loss:1.3091174042984588\n",
      "train loss:1.1863230571577608\n",
      "train loss:1.1698383456800068\n",
      "train loss:1.1009603690368504\n",
      "train loss:1.0986519405506807\n",
      "train loss:1.209807131895016\n",
      "train loss:1.2224709870873482\n",
      "train loss:1.1740658212762953\n",
      "train loss:1.2840268493827927\n",
      "train loss:1.233194203878741\n",
      "train loss:1.3177408984703\n",
      "train loss:0.9835767635757261\n",
      "train loss:1.1632647716327196\n",
      "train loss:1.214240043788309\n",
      "train loss:1.370748499570557\n",
      "train loss:1.2830905673629354\n",
      "train loss:1.2536063502313686\n",
      "train loss:1.275045726514585\n",
      "train loss:1.1350793327037811\n",
      "train loss:1.1209453103261489\n",
      "train loss:1.2712362467651672\n",
      "train loss:1.216909792260548\n",
      "train loss:1.0867726304102903\n",
      "train loss:1.3525227650674976\n",
      "train loss:1.237084413395653\n",
      "train loss:1.1229507180938278\n",
      "train loss:1.1715448892185245\n",
      "train loss:1.3188967897428754\n",
      "train loss:1.1492649913794089\n",
      "train loss:1.2307367416420534\n",
      "train loss:1.1437475554233434\n",
      "train loss:1.2757004599585793\n",
      "train loss:1.0999651626332365\n",
      "train loss:1.2635897469696151\n",
      "train loss:1.1806370191761386\n",
      "train loss:1.1271639239187123\n",
      "train loss:1.0910481257412163\n",
      "train loss:1.1995470597608788\n",
      "train loss:1.0524358690237556\n",
      "train loss:1.2456105429921065\n",
      "train loss:1.1360027699372368\n",
      "train loss:1.239632970449799\n",
      "train loss:1.1331014890372304\n",
      "train loss:1.3273190131039507\n",
      "train loss:1.050387618909878\n",
      "train loss:1.2859106699799818\n",
      "train loss:1.1049415124408468\n",
      "train loss:1.3386885867803742\n",
      "train loss:1.2336682565520114\n",
      "=== epoch:19, train acc:0.906, test acc:0.859 ===\n",
      "train loss:1.3187771256094962\n",
      "train loss:1.082410323421515\n",
      "train loss:1.2357964420742653\n",
      "train loss:1.1585569017576882\n",
      "train loss:1.0425238034992896\n",
      "train loss:1.0347074161977172\n",
      "train loss:1.1921840271009905\n",
      "train loss:1.0583542124588248\n",
      "train loss:1.0718737224885169\n",
      "train loss:1.155636397537141\n",
      "train loss:1.2900925524814435\n",
      "train loss:1.1519683480201588\n",
      "train loss:1.3737930004861532\n",
      "train loss:1.176240854584822\n",
      "train loss:1.1350088642864928\n",
      "train loss:1.0307208614948509\n",
      "train loss:1.037070198804896\n",
      "train loss:1.443656315881625\n",
      "train loss:1.1887170515045118\n",
      "train loss:1.1772025664249512\n",
      "train loss:1.3259870321093872\n",
      "train loss:1.098527004945717\n",
      "train loss:1.1853369041037825\n",
      "train loss:1.0867784963583522\n",
      "train loss:1.1640364288568383\n",
      "train loss:1.4540362953476273\n",
      "train loss:0.9937083911789237\n",
      "train loss:1.399664083938856\n",
      "train loss:1.0304694295314916\n",
      "train loss:1.1192923515856745\n",
      "train loss:1.1632918595176518\n",
      "train loss:1.0651700959169206\n",
      "train loss:1.183647032599449\n",
      "train loss:1.2248368550566664\n",
      "train loss:1.224988814810777\n",
      "train loss:1.1348356440837426\n",
      "train loss:1.0826052479754926\n",
      "train loss:1.0567024091216113\n",
      "train loss:1.2910085067856696\n",
      "train loss:1.3439103620773316\n",
      "train loss:1.0362212093710435\n",
      "train loss:0.9891961855824567\n",
      "train loss:1.1822795947665048\n",
      "train loss:1.1406282312604943\n",
      "train loss:1.32257922960828\n",
      "train loss:1.074215338283933\n",
      "train loss:1.1478564716962578\n",
      "train loss:1.0764020943102448\n",
      "train loss:1.1156557143082246\n",
      "train loss:1.1790284699721532\n",
      "=== epoch:20, train acc:0.907, test acc:0.855 ===\n",
      "train loss:1.221380060348346\n",
      "train loss:1.0996945358425112\n",
      "train loss:1.2174378230777168\n",
      "train loss:1.1311354421319106\n",
      "train loss:1.0316323154360714\n",
      "train loss:1.3076358699676165\n",
      "train loss:1.3502550777600737\n",
      "train loss:1.0915098367683456\n",
      "train loss:1.1809346791751723\n",
      "train loss:0.8886477419318943\n",
      "train loss:1.055695609076695\n",
      "train loss:1.201923234836949\n",
      "train loss:1.2267395406074528\n",
      "train loss:1.0499888628338987\n",
      "train loss:1.3061585132517752\n",
      "train loss:1.061726311637439\n",
      "train loss:1.1953387015541592\n",
      "train loss:1.1505638336452235\n",
      "train loss:1.1325351172079312\n",
      "train loss:1.1546902053066805\n",
      "train loss:1.183945417790326\n",
      "train loss:1.1818749568960443\n",
      "train loss:1.0548289331864151\n",
      "train loss:1.2393639997581127\n",
      "train loss:1.0413822602048697\n",
      "train loss:1.2932772436399653\n",
      "train loss:1.2517804268175015\n",
      "train loss:1.0422963794676934\n",
      "train loss:1.1012459344971446\n",
      "train loss:1.1921675962581282\n",
      "train loss:1.091997091194675\n",
      "train loss:1.1500764885745556\n",
      "train loss:1.0976287474504138\n",
      "train loss:1.153699625734923\n",
      "train loss:1.2110864012595886\n",
      "train loss:1.2986159930140633\n",
      "train loss:1.236824739517325\n",
      "train loss:1.2615448324890663\n",
      "train loss:1.1029262295657087\n",
      "train loss:1.1571832165181828\n",
      "train loss:1.172534311836021\n",
      "train loss:1.0844348736849891\n",
      "train loss:1.2620233111360775\n",
      "train loss:1.2436069668823553\n",
      "train loss:1.0707685920982521\n",
      "train loss:1.2222614141119164\n",
      "train loss:1.0747345256805705\n",
      "train loss:1.1749765610380385\n",
      "train loss:1.1214870826816221\n",
      "=============== Final Test Accuracy ===============\n",
      "test acc:0.851\n",
      "Saved Network Parameters!\n"
     ]
    }
   ],
   "source": [
    "import sys, os\n",
    "sys.path.append(os.pardir)  # 親ディレクトリのファイルをインポートするための設定\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from dataset.mnist import load_mnist\n",
    "from common.trainer import Trainer\n",
    "\n",
    "(x_train, t_train), (x_test, t_test) = load_mnist(flatten=False)\n",
    "\n",
    "# 処理に時間のかかる場合はデータを削減 \n",
    "x_train, t_train = x_train[:5000], t_train[:5000]\n",
    "x_test, t_test = x_test[:1000], t_test[:1000]\n",
    "\n",
    "network = DeepConvNet()  \n",
    "trainer = Trainer(network, x_train, t_train, x_test, t_test,\n",
    "                  epochs=20, mini_batch_size=100,\n",
    "                  optimizer='Adam', optimizer_param={'lr':0.001},\n",
    "                  evaluate_sample_num_per_epoch=1000)\n",
    "trainer.train()\n",
    "\n",
    "# パラメータの保存\n",
    "network.save_params(\"../ch08/deep_convnet_params.pkl\")\n",
    "print(\"Saved Network Parameters!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "max_epochs = 20\n",
    "# グラフの描画\n",
    "markers = {'train': 'o', 'test': 's'}\n",
    "x = np.arange(max_epochs)\n",
    "plt.plot(x, trainer.train_acc_list, marker='o', label='train', markevery=2)\n",
    "plt.plot(x, trainer.test_acc_list, marker='s', label='test', markevery=2)\n",
    "plt.xlabel(\"epochs\")\n",
    "plt.ylabel(\"accuracy\")\n",
    "plt.ylim(0, 1.0)\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ch08/misclassified_mnist.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "sys.path.append(os.pardir)  # 親ディレクトリのファイルをインポートするための設定\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from dataset.mnist import load_mnist\n",
    "\n",
    "\n",
    "(x_train, t_train), (x_test, t_test) = load_mnist(flatten=False)\n",
    "\n",
    "network = DeepConvNet()\n",
    "network.load_params(\"../ch08/deep_convnet_params.pkl\")\n",
    "\n",
    "print(\"calculating test accuracy ... \")\n",
    "#sampled = 1000\n",
    "#x_test = x_test[:sampled]\n",
    "#t_test = t_test[:sampled]\n",
    "\n",
    "classified_ids = []\n",
    "\n",
    "acc = 0.0\n",
    "batch_size = 100\n",
    "\n",
    "for i in range(int(x_test.shape[0] / batch_size)):\n",
    "    tx = x_test[i*batch_size:(i+1)*batch_size]\n",
    "    tt = t_test[i*batch_size:(i+1)*batch_size]\n",
    "    y = network.predict(tx, train_flg=False)\n",
    "    y = np.argmax(y, axis=1)\n",
    "    classified_ids.append(y)\n",
    "    acc += np.sum(y == tt)\n",
    "    \n",
    "acc = acc / x_test.shape[0]\n",
    "print(\"test accuracy:\" + str(acc))\n",
    "\n",
    "classified_ids = np.array(classified_ids)\n",
    "classified_ids = classified_ids.flatten()\n",
    " \n",
    "max_view = 20\n",
    "current_view = 1\n",
    "\n",
    "fig = plt.figure()\n",
    "fig.subplots_adjust(left=0, right=1, bottom=0, top=1, hspace=0.2, wspace=0.2)\n",
    "\n",
    "mis_pairs = {}\n",
    "for i, val in enumerate(classified_ids == t_test):\n",
    "    if not val:\n",
    "        ax = fig.add_subplot(4, 5, current_view, xticks=[], yticks=[])\n",
    "        ax.imshow(x_test[i].reshape(28, 28), cmap=plt.cm.gray_r, interpolation='nearest')\n",
    "        mis_pairs[current_view] = (t_test[i], classified_ids[i])\n",
    "            \n",
    "        current_view += 1\n",
    "        if current_view > max_view:\n",
    "            break\n",
    "\n",
    "print(\"======= misclassified result =======\")\n",
    "print(\"{view index: (label, inference), ...}\")\n",
    "print(mis_pairs)\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ch08/half_float_network.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "sys.path.append(os.pardir)  # 親ディレクトリのファイルをインポートするための設定\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from dataset.mnist import load_mnist\n",
    "\n",
    "\n",
    "(x_train, t_train), (x_test, t_test) = load_mnist(flatten=False)\n",
    "\n",
    "network = DeepConvNet()\n",
    "network.load_params(\"../ch08/deep_convnet_params.pkl\")\n",
    "\n",
    "sampled = 10000 # 高速化のため\n",
    "x_test = x_test[:sampled]\n",
    "t_test = t_test[:sampled]\n",
    "\n",
    "print(\"caluculate accuracy (float64) ... \")\n",
    "print(network.accuracy(x_test, t_test))\n",
    "\n",
    "# float16に型変換\n",
    "x_test = x_test.astype(np.float16)\n",
    "for param in network.params.values():\n",
    "    param[...] = param.astype(np.float16)\n",
    "\n",
    "print(\"caluculate accuracy (float16) ... \")\n",
    "print(network.accuracy(x_test, t_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
